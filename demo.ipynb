{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.916 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "def chinese_word_cut(mytext):\n",
    "    return ' '.join(jieba.cut(mytext))\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file, encoding='utf-8') as f:\n",
    "        stopwords = f.read( )\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic #%d:' % topic_idx)\n",
    "        print(' '.join([feature_names[i]\n",
    "                      for i in topic.argsort()[:-n_top_words -1:-1]]))\n",
    "    print()\n",
    "df = pd.read_csv('data_20180601_20180605.csv', encoding='gb18030')\n",
    "df.columns = ['title', 'author', 'comment'] \n",
    "X = df[['comment']]\n",
    "X['cutted_comment'] = X.comment.apply(chinese_word_cut)\n",
    "stop_words_file = 'mystopwords.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "vect = CountVectorizer()\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X.cutted_comment).toarray(), columns=vect.get_feature_names())\n",
    "vect = CountVectorizer(stop_words=frozenset(stopwords)) #加上停用词去除功能\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X.cutted_comment).toarray(), columns=vect.get_feature_names())\n",
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 3 # 在地狱这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "vect = CountVectorizer(max_df=max_df,\n",
    "                      min_df=min_df,\n",
    "                      token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                      stop_words=frozenset(stopwords))\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X.cutted_comment).toarray(), columns=vect.get_feature_names())\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                               max_features=n_features,\n",
    "                               max_df=max_df,\n",
    "                               min_df=min_df,\n",
    "                               token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                               stop_words=frozenset(stopwords))\n",
    "tf = tf_vectorizer.fit_transform(X.cutted_comment)\n",
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics,\n",
    "                               max_iter=50,\n",
    "                               learning_method='online',\n",
    "                               learning_offset=50.,\n",
    "                               random_state=0)\n",
    "lda.fit(tf)\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、结果：（默认5个主题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、结果：（10个主题）\n",
    "## 其中主题数可以自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10    #此处的10可以自定义\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、结果：（20个主题）\n",
    "## 其中主题数可以自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20    #此处的20可以自定义\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
